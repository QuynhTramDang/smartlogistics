FROM apache/airflow:2.9.3

USER root

# Base packages for Java + Python build
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk wget curl unzip zip ca-certificates \
                       build-essential libssl-dev zlib1g-dev libbz2-dev \
                       libreadline-dev libsqlite3-dev libncursesw5-dev \
                       xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev \
                       liblzma-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Java ENV
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Build Python 3.12 from source
ENV PYTHON_VERSION=3.12.5
RUN cd /tmp && \
    wget https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz && \
    tar xzf Python-${PYTHON_VERSION}.tgz && \
    cd Python-${PYTHON_VERSION} && \
    ./configure --enable-optimizations && \
    make -j$(nproc) && \
    make altinstall && \
    rm -rf /tmp/Python-${PYTHON_VERSION}*

# Copy requirements
COPY dockerfile/requirements-airflow.txt /tmp/requirements.txt

# Upgrade pip & install packages
RUN python3.12 -m ensurepip && \
    python3.12 -m pip install --no-cache-dir --upgrade pip setuptools wheel && \
    python3.12 -m pip install --no-cache-dir -r /tmp/requirements.txt

# Install MinIO client
RUN curl -L -o /usr/local/bin/mc https://dl.min.io/client/mc/release/linux-amd64/mc && \
    chmod +x /usr/local/bin/mc

# Install Spark binary (optional for local driver submit)
ENV SPARK_VERSION=3.4.1
ENV HADOOP_SHORT=3
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_SHORT}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_SHORT}.tgz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_SHORT} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_SHORT}.tgz

# Download Delta Lake & Hadoop AWS jars
ENV DELTA_VERSION=2.4.0
ENV HADOOP_AWS_VERSION=3.3.4
ENV AWS_SDK_VER=1.12.262
RUN mkdir -p /opt/spark/jars && \
    curl -sSL "https://repo1.maven.org/maven2/io/delta/delta-core_2.12/${DELTA_VERSION}/delta-core_2.12-${DELTA_VERSION}.jar" \
        -o /opt/spark/jars/delta-core_2.12-${DELTA_VERSION}.jar && \
    curl -sSL "https://repo1.maven.org/maven2/io/delta/delta-storage/${DELTA_VERSION}/delta-storage-${DELTA_VERSION}.jar" \
        -o /opt/spark/jars/delta-storage-${DELTA_VERSION}.jar && \
    curl -sSL "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar" \
        -o /opt/spark/jars/hadoop-aws-${HADOOP_AWS_VERSION}.jar && \
    curl -sSL "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VER}/aws-java-sdk-bundle-${AWS_SDK_VER}.jar" \
        -o /opt/spark/jars/aws-java-sdk-bundle-${AWS_SDK_VER}.jar

RUN chown -R airflow: /opt/spark

# Final setup
USER airflow
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:/opt/spark/bin
ENV PYTHONPATH="/opt/airflow/jobs:/opt/airflow/dags"
ENV PYSPARK_PYTHON=/usr/local/bin/python3.12
ENV PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3.12
